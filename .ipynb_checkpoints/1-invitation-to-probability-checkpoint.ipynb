{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reasoning Under Uncertainty Workshop\n",
    "# PyCon 2015\n",
    "### Part 1 : An invitation to probability\n",
    "---\n",
    "\n",
    "**Author** : Ronojoy Adhikari   \n",
    "**Email**  : [rjoy@imsc.res.in]() | **Web**    : [www.imsc.res.in/~rjoy]()  \n",
    "**Github** : [www.github.com/ronojoy]() | **Twitter**: @phyrjoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random variables in python\n",
    "\n",
    "> The probability that a random variable $X$ takes on a value $X=x$ is given by $P(x)$\n",
    "\n",
    "- A random variable is not a number, but rather, the pair consisting of the variable and its probability distribution\n",
    "- Probabilities always sum to one : $\\sum_x P(x) = 1$\n",
    "- Random variables can be discrete or continuous, according to the values the random variable can assume.\n",
    "- Discrete random variable - coin tosses, die rolls,  ... \n",
    "- Continuous random variable - rainfall, temperature, financial indices ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# the scipy.stats package has many discrete and continuous random variables\n",
    "\n",
    "# the coin toss random variable\n",
    "X = stats.bernoulli(p=0.5)\n",
    "\n",
    "# bernoulli mean = p, variance = p(1-p)\n",
    "#X.mean(), X.var()  \n",
    "\n",
    "# entropy = -\\sum_x p(x) * ln p(x)\n",
    "# quantifies the uncertainty in the random variable\n",
    "#X.entropy() \n",
    "\n",
    "# lots of throws of the coin\n",
    "x = X.rvs(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the empirical frequency distribution - aka the histogram\n",
    "pylab.rcParams['figure.figsize'] = 6, 4 # medium size figures\n",
    "plt.hist(x, normed = True, color=\"#348ABD\", alpha = 0.4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# the 'central limit theorem' random variable\n",
    "X = stats.norm()\n",
    "\n",
    "# the normal deviate has zero mean and unit variance\n",
    "#X.mean(), X.var()\n",
    "\n",
    "# the normal deviate has the greatest entropy amongst all continuous, unbounded \n",
    "# random variables with given mean and variance\n",
    "# see Jaynes - Probability theory, the logic of science - for proof and discussion\n",
    "\n",
    "#X.entropy()\n",
    "\n",
    "# a million draws from the normal distribution\n",
    "x = X.rvs(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the empirical frequency distribution - aka the histogram\n",
    "pylab.rcParams['figure.figsize'] = 6, 4 # medium size figure\n",
    "plt.hist(x, bins = 50, normed = True, color=\"#348ABD\", alpha = 0.4 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The central limit theorem \n",
    "\n",
    "**Intuitive idea** : add a lot of *independent*, and *identically distributed* random variabes and the result will\n",
    "normally distributed.\n",
    "\n",
    "**More precisely** : Consider *i.i.d* random varibles $X_i$ with mean $\\mu$ and variance $\\sigma^2$. \n",
    "\n",
    "The sum $S_n = \\frac{X_1 + X_2 + \\ldots X_n}{n}$ approaches a normal distribution with mean $\\mu$  and variance $\\frac{\\sigma^2}{n}$ as $n\\rightarrow \\infty$. \n",
    "\n",
    "**Lets see this in Python** ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# lets add lots independent, identically distributed numbers together\n",
    "X = stats.uniform() # uniform deviates\n",
    "X = stats.bernoulli(0.5) # coin tosses\n",
    "X = stats.poisson(1) # telephone calls\n",
    "\n",
    "print 'The mean and variance of the uniform distribtion are', X.mean(), 'and', X.var(), 'respectively.'\n",
    "\n",
    "N = [1, 2, 4, 6, 8, 16, 32, 64]\n",
    "\n",
    "for k, n in enumerate(N):\n",
    "\n",
    "    # draw the n random variables\n",
    "    x = X.rvs(size = [n, 1e5])\n",
    "    \n",
    "    # get their sum \n",
    "    s = x.mean(axis=0)\n",
    "    \n",
    "    # plot the distribution of the sum\n",
    "    sx = plt.subplot(len(N) / 2, 2, k + 1)\n",
    "    plt.setp(sx.get_yticklabels(), visible=False)\n",
    "    plt.hist(s, \n",
    "             bins = 50,\n",
    "             label=\"$\\mu = $ %f \\n $n\\sigma^2 = $ %f \" % (s.mean(), n*s.var()),\n",
    "             color=\"#348ABD\", \n",
    "             alpha=0.4)\n",
    "    \n",
    "    leg = plt.legend()\n",
    "    leg.get_frame().set_alpha(0.4)\n",
    "    plt.autoscale(tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The central limit theorem \n",
    "\n",
    "**Intuitive idea** : add a lot of *independent*, and *identically distributed* random variabes and the result will\n",
    "normally distributed.\n",
    "\n",
    "**More precisely** : Consider *i.i.d* random varibles $X_i$ with mean $\\mu$ and variance $\\sigma^2$. \n",
    "\n",
    "The sum $S_n = \\frac{X_1 + X_2 + \\ldots X_n}{n}$ approaches a normal distribution with mean $\\mu$  and variance $\\frac{\\sigma^2}{n}$ as $n\\rightarrow \\infty$. \n",
    "\n",
    "** The normal distribution ** $P(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ **is important!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Is this a fair coin ? Bayesian inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# N throws of a coin with parameter theta0\n",
    "N, theta0 = 500, 0.5\n",
    "\n",
    "# data contains the outcome of the trial\n",
    "data = stats.bernoulli.rvs(p = theta0, size = N)\n",
    "\n",
    "# theta is distributed in the unit interval\n",
    "theta = np.linspace(0, 1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# inspiration : Devinder Sivia and Cam Pilon\n",
    "# compute posterior after the nthrow-th trial\n",
    "\n",
    "nthrow = [0, 1, 2, 3, 4, 5, 8, 16, 32, N]\n",
    "\n",
    "for k, n in enumerate(nthrow):\n",
    "\n",
    "    # number of heads\n",
    "    heads = data[:n].sum()\n",
    "    \n",
    "    # posterior probability of theta with conjugate prior (see Sivia for derivation)\n",
    "    p = stats.beta.pdf(theta, 1 + heads, 1 + n - heads)\n",
    "\n",
    "    # plot the posterior\n",
    "    sx = plt.subplot(len(nthrow) / 2, 2, k + 1)\n",
    "    plt.setp(sx.get_yticklabels(), visible=False)\n",
    "    plt.plot(theta, p, label=\"tosses %d \\n heads %d \" % (n, heads))\n",
    "    plt.fill_between(theta, 0, p, color=\"#348ABD\", alpha=0.4)\n",
    "    plt.vlines(theta0, 0, 4, color=\"k\", linestyles=\"--\", lw=1)\n",
    "    leg = plt.legend()\n",
    "    leg.get_frame().set_alpha(0.4)\n",
    "    plt.autoscale(tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 1.1\n",
    "\n",
    "- Compute the Bayesian credible intervals which contain 90% of the probability at each iteration and plot the result\n",
    "- Compute the entropy of the posterior distribution at each stage. How do you interpret changing entropy values ? \n",
    "- How would you answer the question \"is this a fair coin\" after this analysis ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why we need probabilistic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = stats.bernoulli(0.5) # one fair coin\n",
    "Y = stats.bernoulli(0.5) # another fair coin\n",
    "\n",
    "# want a new random variable which is their sum\n",
    "Z = Y + X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why you no grok random variables ?\n",
    "\n",
    "![](img/wat-why-you-no-face.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why we need probabilistic programming\n",
    "\n",
    "```\n",
    "# only if inference were this easy\n",
    "theta ~ uniform(0,1)\n",
    "coin ~ bernoulli(theta, N=10)\n",
    "observe(coin.head = 7)\n",
    "infer(theta)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why we need probabilistic programming\n",
    "\n",
    "- Need random variables (i.e. both values and distributions) as objects\n",
    "- Need algebra and calculus of random variables to be automatic\n",
    "- Need conditioning, that is $P(A | B)$ \n",
    "- Need marginalisation, that is $P(A) = \\sum_{B}P(A, B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why we need probabilistic programming\n",
    "\n",
    "**Amazing machine learning and inference applications can be built if programs have probability semantics!**\n",
    "\n",
    "- Microsoft : [Infer.net](http://research.microsoft.com/en-us/um/cambridge/projects/infernet/) \n",
    "- DARPA : [Church](https://projects.csail.mit.edu/church/wiki/Church), [BLOG](http://bayesianlogic.github.io/) ...\n",
    "- Oxford : [Probabilistic C](http://www.robots.ox.ac.uk/~brooks/probabilistic-c/), [Anglican](http://www.robots.ox.ac.uk/~fwood/anglican)\n",
    "- many more contenders, see [probabilistic-programming.org](http://probabilistic-programming.org/wiki/Home)\n",
    "\n",
    "** What's happening in the Python world ? **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
